{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction:\n",
    "\n",
    "In this project, we implement a basic decision tree classifier in Python from scratch. A decision tree is a widely used machine learning algorithm that is particularly effective for classification tasks. It builds a model in the form of a tree structure, where each internal node represents a test on an attribute, each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes).\n",
    "\n",
    "The decision tree classifier works by recursively splitting the dataset into subsets based on the attribute that provides the highest information gain, a measure of how well an attribute separates the data into distinct classes. The process continues until the subsets are homogeneous enough (i.e., all instances belong to the same class) or other stopping criteria are met.\n",
    "\n",
    "This code is organized into several key components:\n",
    "\n",
    "- TreeNode Class: Defines the structure of each node in the decision tree.\n",
    "- Helper Functions: Includes functions for reading data from a CSV file, calculating entropy and information gain, and selecting the best attribute for splitting the data.\n",
    "- Tree Building Function: Recursively constructs the decision tree from the dataset.\n",
    "- Tree Printing Function: Provides a way to visually inspect the structure of the constructed tree.\n",
    "- Classification Function: Classifies new instances using the constructed decision tree.\n",
    "- Main Function: Orchestrates the entire process by reading input data, building the tree, and classifying new instances.\n",
    "\n",
    "This implementation provides a practical and educational tool for understanding the inner workings of decision trees, from reading and processing data to building and utilizing a decision tree for classification. It is designed to be simple and intuitive, making it a great starting point for anyone interested in learning about decision trees and machine learning in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Algorithm Descriptions:\n",
    "\n",
    "### Introduction to the ID3 Algorithm\n",
    "\n",
    "The ID3 (Iterative Dichotomiser 3) algorithm is a popular algorithm used to create decision trees, a type of predictive model used in machine learning for classification tasks. Developed by Ross Quinlan in 1986, ID3 is designed to construct a decision tree by employing a top-down, greedy approach. It uses information gain as a criterion to select the attribute that best separates the data into distinct classes at each step of the tree construction.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "**1. Entropy:**\n",
    "Entropy is a measure of the randomness or impurity in a dataset. In the context of decision trees, it quantifies the degree of uncertainty or disorder in the target variable (class labels). The entropy \\( H \\) of a dataset is given by:\n",
    "\n",
    "$$\n",
    "   \\ H(S) = -\\sum_{i=1}^{n} p_i \\log_2(p_i) \\\n",
    "$$\n",
    "\n",
    "where \\( p_i \\) is the proportion of instances belonging to class \\( i \\).\n",
    "\n",
    "**2. Information Gain:**\n",
    "Information gain measures the reduction in entropy achieved by partitioning the data based on a particular attribute. It quantifies how well a given attribute separates the data into classes. The information gain \\( IG \\) of an attribute \\( A \\) is calculated as:\n",
    "\n",
    "$$\n",
    "   \\ IG(S, A) = H(S) - \\sum_{v \\in \\text{Values}(A)} \\frac{|S_v|}{|S|} H(S_v) \\\n",
    "$$\n",
    " \n",
    "where \\( S \\) is the dataset, \\( S_v \\) is the subset of \\( S \\) where attribute \\( A \\) has value \\( v \\), and \\( H(S_v) \\) is the entropy of the subset \\( S_v \\).\n",
    "\n",
    "### The ID3 Algorithm\n",
    "\n",
    "The ID3 algorithm follows these steps:\n",
    "\n",
    "1. **Start with the Entire Dataset:**\n",
    "   Begin with the root node representing the entire dataset.\n",
    "\n",
    "2. **Calculate Entropy:**\n",
    "   Calculate the entropy of the current dataset.\n",
    "\n",
    "3. **Select the Best Attribute:**\n",
    "   For each attribute, calculate the information gain. Select the attribute with the highest information gain as the splitting criterion.\n",
    "\n",
    "4. **Split the Data:**\n",
    "   Partition the dataset into subsets based on the selected attribute's values.\n",
    "\n",
    "5. **Create Child Nodes:**\n",
    "   Create a child node for each subset and assign the corresponding subset to the child node.\n",
    "\n",
    "6. **Repeat Recursively:**\n",
    "   Repeat the process for each child node using the subset of data associated with that node, excluding the previously used attribute.\n",
    "\n",
    "7. **Stopping Criteria:**\n",
    "   The recursion stops when one of the following conditions is met:\n",
    "   - All instances in a subset belong to the same class.\n",
    "   - There are no more attributes to split on.\n",
    "   - The subset is empty.\n",
    "\n",
    "8. **Assign Class Labels:**\n",
    "   Assign a class label to leaf nodes based on the majority class in the subset of data.\n",
    "\n",
    "### Example and Practical Use\n",
    "\n",
    "Consider a dataset where we want to predict whether a customer will buy a computer based on attributes like age, income, student status, and credit rating. Using the ID3 algorithm, we would:\n",
    "\n",
    "1. Calculate the initial entropy of the dataset.\n",
    "2. Compute the information gain for each attribute.\n",
    "3. Select the attribute with the highest information gain (e.g., 'age').\n",
    "4. Split the dataset into subsets based on the selected attribute's values (e.g., 'youth', 'middle-aged', 'senior').\n",
    "5. Repeat the process for each subset, recursively building the tree until the stopping criteria are met.\n",
    "\n",
    "### Advantages and Limitations\n",
    "\n",
    "**Advantages:**\n",
    "- **Simple and Intuitive:** The ID3 algorithm is easy to understand and implement.\n",
    "- **Effective for Small to Medium-sized Datasets:** It performs well on datasets with a manageable number of attributes and instances.\n",
    "- **No Need for Feature Scaling:** ID3 handles both categorical and numerical data without requiring scaling.\n",
    "\n",
    "**Limitations:**\n",
    "- **Overfitting:** The algorithm may produce a complex tree that overfits the training data, especially if the tree is deep.\n",
    "- **Bias Towards Attributes with More Levels:** ID3 tends to favor attributes with more distinct values, which might not always be the best choice.\n",
    "- **Missing Values:** The algorithm does not handle missing values inherently.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The ID3 algorithm is a foundational method for constructing decision trees, serving as a basis for more advanced algorithms like C4.5 and CART. Its simplicity and effectiveness make it a valuable tool for educational purposes and for solving basic classification problems. Understanding ID3 provides a solid grounding in decision tree learning and the principles of information theory applied in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The `TreeNode` Class\n",
    "\n",
    "In this section, we will explore the `TreeNode` class, which is a crucial component of our decision tree implementation. This class represents a node in the decision tree, encapsulating the attributes, children, and class labels necessary for making decisions.\n",
    "\n",
    "\n",
    "### Defining the `TreeNode` Class\n",
    "\n",
    "The `TreeNode` class has several attributes and methods that help in building and navigating the decision tree.\n",
    "\n",
    "```python\n",
    "class TreeNode:\n",
    "    def __init__(self, attribute=None):\n",
    "        self.attribute = attribute  # Attribute used for splitting at this node\n",
    "        self.children = {}  # Dictionary to store children nodes\n",
    "        self.class_label = None  # Class label for leaf nodes\n",
    "        self.counter = Counter()  # Counter to keep track of class label occurrences\n",
    "```\n",
    "\n",
    "### Attributes\n",
    "\n",
    "- `attribute`: This is the attribute used for splitting the data at this node. For internal nodes, it determines which feature is used to partition the dataset.\n",
    "\n",
    "- `children`: A dictionary where the keys are the possible values of the splitting attribute, and the values are the child nodes resulting from the split.\n",
    "\n",
    "- `class_label`: This is the class label assigned to the node. It is `None` for internal nodes and holds the predicted class for leaf nodes.\n",
    "\n",
    "- `counter`: A `Counter` object from the `collections` module to keep track of the frequency of class labels in the dataset reaching this node. This helps in making predictions and understanding the distribution of the data.\n",
    "\n",
    "### Initialization\n",
    "\n",
    "The `__init__` method initializes the node with the given attribute (default is `None`). It also initializes an empty dictionary for child nodes, sets the class label to `None`, and initializes a `Counter` to keep track of class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv  # Import the CSV module to handle CSV file operations\n",
    "import math  # Import the math module to perform mathematical operations\n",
    "from collections import Counter  # Import Counter from collections module to count occurrences of elements\n",
    "\n",
    "# Define a class for tree nodes used in the decision tree\n",
    "class TreeNode:\n",
    "    def __init__(self, attribute=None):\n",
    "        self.attribute = attribute  # Attribute used for splitting at this node\n",
    "        self.children = {}  # Dictionary to store children nodes\n",
    "        self.class_label = None  # Class label for leaf nodes\n",
    "        self.counter = Counter()  # Counter to keep track of class label occurrences\n",
    "\n",
    "# Function to read CSV file and return the data as a list of dictionaries\n",
    "def read_csv(file_path):\n",
    "    data = []  # Initialize an empty list to store the data\n",
    "    with open(file_path, 'r') as file:  # Open the CSV file in read mode\n",
    "        reader = csv.DictReader(file)  # Create a CSV reader object to read the file as a dictionary\n",
    "        for row in reader:  # Iterate over each row in the CSV file\n",
    "            cleaned_row = {key: value for key, value in row.items() if key != 'ID'}  # Skip the 'ID' column\n",
    "            data.append(cleaned_row)  # Append the cleaned row to the data list\n",
    "    return data  # Return the list of dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Entropy Calculation\n",
    "\n",
    "In this section, we will explore the `entropy` function, which calculates the entropy of a dataset based on the class labels. Entropy is a measure of impurity or disorder in the dataset, and it helps in determining the effectiveness of attribute splits in decision tree algorithms.\n",
    "\n",
    "## Defining the `entropy` Function\n",
    "\n",
    "The `entropy` function calculates the entropy of a dataset based on the class labels.\n",
    "\n",
    "```python\n",
    "def entropy(data, class_label):\n",
    "    if not data:\n",
    "        return 0\n",
    "    label_column = [row[class_label] for row in data]\n",
    "    label_counts = Counter(label_column)\n",
    "    entropy_val = 0\n",
    "    total_examples = len(data)\n",
    "    for label in label_counts:\n",
    "        label_prob = label_counts[label] / total_examples\n",
    "        entropy_val -= label_prob * math.log2(label_prob)\n",
    "    return entropy_val\n",
    "```\n",
    "\n",
    "### Explanation\n",
    "\n",
    "- **data**: The dataset for which entropy needs to be calculated.\n",
    "- **class_label**: The column name representing the class labels in the dataset.\n",
    "- **label_column**: A list containing the class labels of all instances in the dataset.\n",
    "- **label_counts**: A `Counter` object that counts the occurrences of each class label in the dataset.\n",
    "- **total_examples**: The total number of instances in the dataset.\n",
    "- **entropy_val**: Initialized to 0, it accumulates the entropy calculation.\n",
    "- **for label in label_counts**: Iterates over each unique class label.\n",
    "- **label_prob**: Calculates the probability of each class label occurrence in the dataset.\n",
    "- **entropy_val**: Updates the entropy calculation based on the probability of each class label occurrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(data, class_label):\n",
    "    if not data:  # If data is empty, return 0\n",
    "        return 0\n",
    "    label_column = [row[class_label] for row in data]  # Extract the class label column\n",
    "    label_counts = Counter(label_column)  # Count occurrences of each class label\n",
    "    entropy_val = 0  # Initialize entropy value\n",
    "    total_examples = len(data)  # Get the total number of examples in the data\n",
    "    for label in label_counts:  # Iterate over each class label\n",
    "        label_prob = label_counts[label] / total_examples  # Calculate the probability of the class label\n",
    "        entropy_val -= label_prob * math.log2(label_prob)  # Update the entropy value\n",
    "    return entropy_val  # Return the calculated entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Information Gain Calculation\n",
    "\n",
    "In this section, we will explore the `information_gain` function, which calculates the information gain of an attribute in a dataset. Information gain helps in selecting the best attribute for splitting the dataset in decision tree algorithms.\n",
    "\n",
    "## Defining the `information_gain` Function\n",
    "\n",
    "The `information_gain` function calculates the information gain of an attribute based on the class labels.\n",
    "\n",
    "```python\n",
    "def information_gain(data, attribute, class_label):\n",
    "    attribute_values = set([row[attribute] for row in data])\n",
    "    total_examples = len(data)\n",
    "    attribute_entropy = 0\n",
    "    for value in attribute_values:\n",
    "        subset = [row for row in data if row[attribute] == value]\n",
    "        subset_entropy = entropy(subset, class_label)\n",
    "        subset_size = len(subset)\n",
    "        attribute_entropy += (subset_size / total_examples) * subset_entropy\n",
    "    return entropy(data, class_label) - attribute_entropy\n",
    "```\n",
    "\n",
    "### Explanation\n",
    "\n",
    "- **data**: The dataset for which information gain needs to be calculated.\n",
    "- **attribute**: The attribute for which information gain is calculated.\n",
    "- **class_label**: The column name representing the class labels in the dataset.\n",
    "- **attribute_values**: A set containing all unique values of the attribute.\n",
    "- **total_examples**: The total number of instances in the dataset.\n",
    "- **attribute_entropy**: Initialized to 0, it accumulates the entropy of subsets based on attribute values.\n",
    "- **for value in attribute_values**: Iterates over each unique value of the attribute.\n",
    "- **subset**: Filters the dataset to include only instances with the current attribute value.\n",
    "- **subset_entropy**: Calculates the entropy of the subset using the `entropy` function.\n",
    "- **subset_size**: The number of instances in the subset.\n",
    "- **attribute_entropy**: Updates the attribute entropy based on the subset entropy and size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the information gain of an attribute\n",
    "def information_gain(data, attribute, class_label):\n",
    "    attribute_values = set([row[attribute] for row in data])  # Get unique values of the attribute\n",
    "    total_examples = len(data)  # Get the total number of examples in the data\n",
    "    attribute_entropy = 0  # Initialize the attribute entropy\n",
    "    for value in attribute_values:  # Iterate over each unique value of the attribute\n",
    "        subset = [row for row in data if row[attribute] == value]  # Get the subset of data with the attribute value\n",
    "        subset_entropy = entropy(subset, class_label)  # Calculate the entropy of the subset\n",
    "        subset_size = len(subset)  # Get the size of the subset\n",
    "        attribute_entropy += (subset_size / total_examples) * subset_entropy  # Update the attribute entropy\n",
    "    return entropy(data, class_label) - attribute_entropy  # Return the information gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Choose Best Attribute for Splitting\n",
    "\n",
    "In this section, we will explore the `choose_best_attribute` function, which selects the best attribute for splitting the dataset based on the highest information gain. This process is crucial in decision tree algorithms for determining the optimal attribute to partition the data.\n",
    "\n",
    "## Defining the `choose_best_attribute` Function\n",
    "\n",
    "The `choose_best_attribute` function selects the best attribute for splitting the dataset based on the highest information gain.\n",
    "\n",
    "```python\n",
    "def choose_best_attribute(data, attributes, class_label):\n",
    "    gains = {attr: information_gain(data, attr, class_label) for attr in attributes}\n",
    "    best_attribute = max(gains, key=gains.get)\n",
    "    return best_attribute\n",
    "```\n",
    "\n",
    "### Explanation\n",
    "\n",
    "- **data**: The dataset for which the best attribute needs to be selected.\n",
    "- **attributes**: A list of attributes available for splitting.\n",
    "- **class_label**: The column name representing the class labels in the dataset.\n",
    "- **gains**: A dictionary comprehension that calculates the information gain for each attribute.\n",
    "- **best_attribute**: Finds the attribute with the highest information gain using the `max` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to choose the best attribute for splitting the data\n",
    "def choose_best_attribute(data, attributes, class_label):\n",
    "    gains = {attr: information_gain(data, attr, class_label) for attr in attributes}  # Calculate information gain for each attribute\n",
    "    best_attribute = max(gains, key=gains.get)  # Find the attribute with the maximum information gain\n",
    "    return best_attribute  # Return the best attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Building the Decision Tree\n",
    "\n",
    "In this section, we will explore the `build_tree` function, which constructs a decision tree recursively by selecting the best attribute for splitting at each node. This process is fundamental in decision tree algorithms for creating an optimal tree structure for classification tasks.\n",
    "\n",
    "## Defining the `build_tree` Function\n",
    "\n",
    "The `build_tree` function constructs a decision tree recursively by selecting the best attribute for splitting at each node.\n",
    "\n",
    "```python\n",
    "def build_tree(data, attributes, class_label):\n",
    "    if not data:\n",
    "        return None\n",
    "\n",
    "    # Create a new tree node\n",
    "    node = TreeNode()\n",
    "\n",
    "    # Check if all examples have the same class label\n",
    "    class_column = [row[class_label] for row in data]\n",
    "    if len(set(class_column)) == 1:\n",
    "        node.class_label = class_column[0]\n",
    "        node.counter = Counter(class_column)\n",
    "        return node\n",
    "\n",
    "    # Choose the best attribute to split on\n",
    "    best_attribute = choose_best_attribute(data, attributes, class_label)\n",
    "    node.attribute = best_attribute\n",
    "\n",
    "    # Split data based on the chosen attribute\n",
    "    attribute_values = set([row[best_attribute] for row in data])\n",
    "    for value in attribute_values:\n",
    "        subset = [row for row in data if row[best_attribute] == value]\n",
    "        child_node = build_tree(subset, [attr for attr in attributes if attr != best_attribute], class_label)\n",
    "        node.children[value] = child_node\n",
    "\n",
    "    return node\n",
    "```\n",
    "\n",
    "### Explanation\n",
    "\n",
    "- **data**: The dataset for which the decision tree needs to be constructed.\n",
    "- **attributes**: A list of attributes available for splitting.\n",
    "- **class_label**: The column name representing the class labels in the dataset.\n",
    "- **node**: Represents a node in the decision tree.\n",
    "- **class_column**: Extracts the class labels from the dataset.\n",
    "- **if len(set(class_column)) == 1**: Checks if all instances have the same class label. If true, creates a leaf node with the class label.\n",
    "- **best_attribute**: Selects the best attribute for splitting using the `choose_best_attribute` function.\n",
    "- **attribute_values**: Extracts unique values of the best attribute.\n",
    "- **for value in attribute_values**: Iterates over each unique value of the best attribute and recursively builds child nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build the decision tree\n",
    "def build_tree(data, attributes, class_label):\n",
    "    if not data:  # If data is empty, return None\n",
    "        return None\n",
    "\n",
    "    node = TreeNode()  # Create a new tree node\n",
    "\n",
    "    class_column = [row[class_label] for row in data]  # Extract the class label column\n",
    "    if len(set(class_column)) == 1:  # If all examples have the same class label\n",
    "        node.class_label = class_column[0]  # Set the class label of the node\n",
    "        node.counter = Counter(class_column)  # Set the counter for the class label\n",
    "        return node  # Return the node\n",
    "\n",
    "    best_attribute = choose_best_attribute(data, attributes, class_label)  # Choose the best attribute to split on\n",
    "    node.attribute = best_attribute  # Set the attribute of the node\n",
    "\n",
    "    attribute_values = set([row[best_attribute] for row in data])  # Get unique values of the best attribute\n",
    "    for value in attribute_values:  # Iterate over each unique value of the best attribute\n",
    "        subset = [row for row in data if row[best_attribute] == value]  # Get the subset of data with the attribute value\n",
    "        child_node = build_tree(subset, [attr for attr in attributes if attr != best_attribute], class_label)  # Build the subtree\n",
    "        node.children[value] = child_node  # Add the subtree as a child of the current node\n",
    "\n",
    "    return node  # Return the node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Printing the Decision Tree\n",
    "\n",
    "In this section, we will explore the `print_tree` function, which prints the structure of a decision tree in a readable format. This visualization helps in understanding the decisions made at each node of the tree.\n",
    "\n",
    "## Defining the `print_tree` Function\n",
    "\n",
    "The `print_tree` function prints the structure of the decision tree.\n",
    "\n",
    "```python\n",
    "def print_tree(node, depth=0):\n",
    "    if node.class_label:\n",
    "        print(f\"{'    ' * depth}<Leaf> {node.class_label} ({', '.join(f'{k}: {v}' for k, v in node.counter.items())})\")\n",
    "    else:\n",
    "        print(f\"{'    ' * depth}<{node.attribute}>\")\n",
    "        for value, child_node in node.children.items():\n",
    "            print(f\"{'    ' * (depth + 1)}{value}: \", end='')\n",
    "            print_tree(child_node, depth + 2)\n",
    "```\n",
    "\n",
    "### Explanation\n",
    "\n",
    "- **node**: The current node in the decision tree.\n",
    "- **depth**: The current depth in the tree, used for indentation.\n",
    "- **if node.class_label**: Checks if the node is a leaf node. If true, prints the class label and the count of class labels.\n",
    "- **else**: If the node is not a leaf, prints the attribute and recursively prints its children.\n",
    "- **print(f\"{'    ' * depth}<{node.attribute}>\")**: Prints the attribute of the current node with indentation based on depth.\n",
    "- **for value, child_node in node.children.items()**: Iterates over each child node and prints its value, then recursively calls `print_tree` on the child node with increased depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(node, depth=0):\n",
    "    if node.class_label:  # If the node is a leaf node\n",
    "        print(f\"{'    ' * depth}<Leaf> {node.class_label} ({', '.join(f'{k}: {v}' for k, v in node.counter.items())})\")  # Print the class label and counter\n",
    "    else:  # If the node is an internal node\n",
    "        print(f\"{'    ' * depth}<{node.attribute}>\")  # Print the attribute\n",
    "        for value, child_node in node.children.items():  # Iterate over each child node\n",
    "            print(f\"{'    ' * (depth + 1)}{value}: \", end='')  # Print the attribute value\n",
    "            print_tree(child_node, depth + 2)  # Recursively print the child node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 9. Classifying Instances with the Decision Tree\n",
    "\n",
    "In this section, we will explore the `classify` function, which predicts the class label of an instance using a decision tree. This process involves traversing the decision tree based on the attribute values of the instance until a leaf node is reached, which provides the predicted class label.\n",
    "\n",
    "## Defining the `classify` Function\n",
    "\n",
    "The `classify` function predicts the class label of an instance using a decision tree.\n",
    "\n",
    "```python\n",
    "def classify(instance, node):\n",
    "    if node.class_label:\n",
    "        return node.class_label\n",
    "\n",
    "    attribute_value = instance.get(node.attribute)\n",
    "    if attribute_value is None or attribute_value not in node.children:\n",
    "        # Handle unknown attribute values by returning a default prediction\n",
    "        # Modify this based on your specific dataset and requirements\n",
    "        return \"Unknown\"\n",
    "\n",
    "    return classify(instance, node.children[attribute_value])\n",
    "```\n",
    "\n",
    "### Explanation\n",
    "\n",
    "- **instance**: The instance for which the class label needs to be predicted.\n",
    "- **node**: The current node in the decision tree.\n",
    "- **if node.class_label**: Checks if the current node is a leaf node. If true, returns the class label of the node.\n",
    "- **attribute_value**: Retrieves the value of the attribute corresponding to the current node from the instance.\n",
    "- **if attribute_value is None or attribute_value not in node.children**: Checks if the attribute value is unknown or not present in the decision tree. In such cases, returns a default prediction (e.g., \"Unknown\").\n",
    "- **return classify(instance, node.children[attribute_value])**: Recursively traverses the decision tree based on the attribute values of the instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to classify a new instance using the decision tree\n",
    "def classify(instance, node):\n",
    "    if node.class_label:  # If the node is a leaf node\n",
    "        return node.class_label  # Return the class label\n",
    "\n",
    "    attribute_value = instance.get(node.attribute)  # Get the attribute value of the instance\n",
    "    if attribute_value is None or attribute_value not in node.children:  # If the attribute value is unknown\n",
    "        return \"Unknown\"  # Return \"Unknown\"\n",
    "\n",
    "    return classify(instance, node.children[attribute_value])  # Recursively classify the instance using the child node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Decision Tree Main Function\n",
    "\n",
    "In this notebook, we'll explore the `main` function, which orchestrates the process of building a decision tree from a dataset and classifying new instances using the constructed tree.\n",
    "\n",
    "## Imports\n",
    "\n",
    "Before diving into the `main` function, let's import the necessary modules:\n",
    "\n",
    "```python\n",
    "import csv\n",
    "import math\n",
    "from collections import Counter\n",
    "```\n",
    "\n",
    "## Defining the `main` Function\n",
    "\n",
    "The `main` function is the entry point of the decision tree construction and classification process. It reads a dataset from a CSV file, constructs a decision tree, prints the tree structure, and classifies new instances using the constructed tree.\n",
    "\n",
    "```python\n",
    "def main():\n",
    "    # Prompt the user to enter the path to the CSV file\n",
    "    file_path = input(\"Enter the path to the CSV file: \")\n",
    "    \n",
    "    # Read the dataset from the CSV file\n",
    "    data = read_csv(file_path)\n",
    "\n",
    "    # Extract attribute names from the header\n",
    "    attributes = list(data[0].keys())  # Extract all attribute names from the first row of data\n",
    "    \n",
    "    # Remove the 'ID' column if present\n",
    "    if 'ID' in attributes:\n",
    "        attributes.remove('ID')\n",
    "    \n",
    "    # Extract the class label column\n",
    "    class_label = attributes[-1]  # Last remaining column is assumed to be the class label\n",
    "    attributes = attributes[:-1]  # Exclude the class label column\n",
    "\n",
    "    # Build the decision tree\n",
    "    root_node = build_tree(data, attributes, class_label)\n",
    "\n",
    "    # Print the decision tree structure\n",
    "    print(\"Decision Tree:\")\n",
    "    print_tree(root_node)\n",
    "\n",
    "    # Prompt the user to enter attribute values for a new instance\n",
    "    new_instance = {}\n",
    "    for attribute in attributes:\n",
    "        value = input(f\"Enter value for '{attribute}': \")\n",
    "        new_instance[attribute] = value\n",
    "\n",
    "    # Classify the new instance using the decision tree\n",
    "    predicted_class = classify(new_instance, root_node)\n",
    "    print(f\"Predicted class for the new instance: {predicted_class}\")\n",
    "\n",
    "# Execute the main function if the script is run directly\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "### Explanation\n",
    "\n",
    "- **Prompting for CSV file path**: Asks the user to enter the path to the CSV file containing the dataset.\n",
    "- **Reading the dataset**: Uses the `read_csv` function to read the dataset from the CSV file.\n",
    "- **Extracting attributes and class label**: Extracts attribute names and the class label from the dataset.\n",
    "- **Building the decision tree**: Constructs a decision tree using the `build_tree` function.\n",
    "- **Printing the decision tree**: Prints the structure of the decision tree using the `print_tree` function.\n",
    "- **Prompting for attribute values**: Prompts the user to enter attribute values for a new instance.\n",
    "- **Classifying the new instance**: Classifies the new instance using the constructed decision tree and the `classify` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    file_path = input(\"Enter the path to the CSV file: \")  # Prompt user to enter the path to the CSV file\n",
    "    data = read_csv(file_path)  # Read the CSV file and get the data\n",
    "\n",
    "    attributes = list(data[0].keys())  # Extract attribute names from the header\n",
    "    if 'ID' in attributes:  # If 'ID' is in the list of attributes\n",
    "        attributes.remove('ID')  # Remove 'ID' from the list of attributes\n",
    "\n",
    "    class_label = attributes[-1]  # Last remaining column is assumed to be the class label\n",
    "    attributes = attributes[:-1]  # Exclude the class label column\n",
    "\n",
    "    root_node = build_tree(data, attributes, class_label)  # Build the decision tree\n",
    "    print(\"Decision Tree:\")  # Print a message\n",
    "    print_tree(root_node)  # Print the decision tree\n",
    "\n",
    "    new_instance = {}  # Initialize an empty dictionary for the new instance\n",
    "    for attribute in attributes:  # Iterate over each attribute\n",
    "        value = input(f\"Enter value for '{attribute}': \")  # Prompt user to enter the value for the attribute\n",
    "        new_instance[attribute] = value  # Add the attribute value to the new instance\n",
    "\n",
    "    predicted_class = classify(new_instance, root_node)  # Classify the new instance using the decision tree\n",
    "    print(f\"Predicted class for the new instance: {predicted_class}\")  # Print the predicted class\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()  # Execute the main function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Conclusion\n",
    "\n",
    "In conclusion, the code presented implements a decision tree classifier from scratch using Python. Here's a summary of its key components and functionalities:\n",
    "\n",
    "1. **Data Handling**: The code reads a dataset from a CSV file, where each row represents an instance with multiple attributes and a class label.\n",
    "\n",
    "2. **Decision Tree Construction**: It constructs a decision tree recursively using the ID3 algorithm. At each node, it selects the best attribute for splitting based on information gain and continues partitioning the dataset until it reaches leaf nodes with homogeneous class labels or stops due to certain conditions.\n",
    "\n",
    "3. **Decision Tree Visualization**: The code provides a function to print the structure of the constructed decision tree in a readable format, allowing users to understand the decision-making process at each node.\n",
    "\n",
    "4. **Instance Classification**: Once the decision tree is built, the code allows users to input attribute values for new instances interactively. It then predicts the class label of these instances using the constructed decision tree.\n",
    "\n",
    "5. **Customization**: The code is flexible and can be adapted to different datasets by modifying the handling of unknown attribute values and other specific requirements.\n",
    "\n",
    "Overall, this code provides a foundational understanding of decision tree algorithms and demonstrates how they can be implemented for classification tasks. It serves as a valuable learning tool for those interested in machine learning algorithms and data analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
